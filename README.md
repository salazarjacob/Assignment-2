# Assignment-2
TThere are a lot of biases to test for in the Perspective API, like gender bias, language bias, etc. I have decided to test the Perspective API for bias against the LGBTQIA+ community. Through exploration of the data, I concluded that Perspective generally labels comments with a score of 0.56 or higher as “toxic”. I ignored all other labels just for ease of understanding. Most comments with the “toxic” label had high scores, and are less likely, in my opinion, to be contested as to being not toxic. Specifically, will comments using words historically used against the community be scored higher or lower based on context, i.e., will comments condemning the community score higher than comments embracing.  Since the scores attributed to a comment represent how “toxic” it may be perceived my someone, I have chosen threshold of ≥0.56 to represent an objectively “high” score, and a threshold of ≤0.55 to represent an objectively “low” score. I have also decided to examine all labels in my analysis. The whole point of the analysis is to see whether certain scores are appropriate for the comment analyzed, so it is essential that all labels be included in the analysis. Excluding any label will skew results and will not misrepresent the Perspective API’s performance ability.
# Hypothesis
Comments with discriminating language against the LGBTQIA+ community will score higher, while comments with similar, but embracing language will also score higher, even though they are subjectively “less” toxic.
